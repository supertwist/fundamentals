*Note: this glossary was generated by ChatGPT!*

# A
**Alignment** - Ensuring a model’s behavior aligns with human intentions and ethical guidelines, critical in safe AI deployment.

**Artificial Intelligence (AI)** - The simulation of human intelligence in machines that can perform tasks typically requiring human intelligence, such as visual perception, speech recognition, decision-making, and language translation.

**Attention Mechanism** - A technique used in neural networks to focus on certain parts of an input sequence, especially useful in sequence-to-sequence tasks.

# B
**Bias** - Systematic errors or unfair advantages/disadvantages learned by a model, often due to unbalanced or incomplete training data.

**Bias Mitigation** - Techniques used to reduce or eliminate biases within a model, often crucial for generative AI.

# C
**Context Window** - The length of text or input data a model can process at once, affecting its performance on tasks with long inputs.

# D
**Data Augmentation** - Techniques for increasing the diversity of a training dataset by modifying data in various ways, such as flipping images or altering text.

**Decoder** - Part of a neural network, typically in sequence-to-sequence models, that generates output from encoded data.

**Deep Learning** - A subset of ML using neural networks with many layers (deep neural networks) to model complex patterns in large datasets.

**Diffusion Model** - A type of generative model that learns to reverse a noise process to generate new data, often used for image synthesis.

# E
**Embedding** - The representation of words, sentences, or other data types as vectors, enabling a model to process text in numerical form.

**Ethics and Bias in Generative AI** - Concerns and best practices for managing issues like fairness, misinformation, and unintended consequences.

**Ethics in AI** - Considerations around the societal, moral, and legal implications of deploying AI, especially generative models.

# F
**Few-Shot Learning** - Training a model on a task with only a few examples, often seen in generative models that perform well with limited data.

**Fine-tuning** - The process of adjusting a pre-trained model on a smaller, task-specific dataset to improve its performance on that task.

# G
**Generative Adversarial Network (GAN)** - A generative model framework with two neural networks (generator and discriminator) competing in a zero-sum game to improve data generation.

**Generative Model** - A type of model that learns to generate new data that resembles a training dataset, commonly used in creating images, text, audio, or video.

**Gradient Descent** - An optimization algorithm used to minimize a model’s error by iteratively adjusting weights in the neural network.

**GPT (Generative Pre-trained Transformer)** - A type of LLM developed by OpenAI that generates text by predicting subsequent words based on context.

# H
**Hallucination** - When a generative model produces false or misleading information confidently, despite lacking factual basis.

**Hyperparameters** - Configurable settings that control the model training process, such as learning rate, batch size, and number of layers.

# I
**Inpainting** - A technique where a model fills in missing parts of an image or text based on surrounding content.

# L
**Large Language Model (LLM)** - A model trained on vast amounts of text data, enabling it to generate human-like language and understand context. Examples include GPT, BERT, and T5.

**Latent Space** - A representation space where a model encodes input data, allowing relationships and patterns to be mapped in lower dimensions.

# M
**Machine Learning (ML)** - A subset of AI focused on using algorithms and statistical models to enable computers to perform specific tasks without explicit programming by learning from data.

**Multimodal Model** - A model that can process and generate multiple types of data, such as text and images, often simultaneously.

# N
**Natural Language Processing (NLP)** - A field of AI focused on the interaction between computers and human language, enabling tasks like language translation, text generation, and sentiment analysis.

**Neural Network** - A model inspired by the human brain’s network of neurons, consisting of layers of nodes (neurons) that can process and recognize patterns in data.

# 0
**Overfitting** - When a model learns the training data too well, including noise, leading to poor generalization on new data.

# P
**Personalization** - Adjusting a model’s outputs to suit individual users’ preferences or characteristics, enhancing user experience.

**Pre-training** - The process of training a model on a large dataset in an unsupervised manner before fine-tuning on a specific task.

**Prompt** - An initial input or question used to instruct or guide a generative AI model to produce specific outputs.

# R
**Reinforcement Learning from Human Feedback (RLHF)** - A training method that incorporates human preferences to improve model responses.

# S
**Sampling** - The process of generating new data points by selecting from a probability distribution produced by a generative model.

**Self-Attention** - A mechanism where each part of an input sequence interacts with all other parts to understand dependencies, used widely in transformers.

# T
**Temperature** - A parameter controlling the randomness of a model’s predictions, with higher values increasing creativity but lowering coherence.

**Tokenization** - The process of splitting text into smaller units, or tokens, for a model to process.

**Transformer** - A neural network architecture designed for handling sequential data, like language, using self-attention mechanisms. Popular for text generation tasks.

# V
**Variational Autoencoder (VAE)** - A generative model that encodes data to a lower-dimensional space and decodes it, allowing the generation of new data instances that resemble the training data.

# Z
**Zero-Shot Learning** - When a model performs a task without being explicitly trained on it by leveraging general understanding from diverse training data.
