# GenAI Glossary
*Note: this glossary was generated by ChatGPT 4o! (with some editing and commentary from James H.)*
*Trust it ONLY as far as you can throw it.*

*Notes to students: I created this as an exercise to see how useful ChatGPT might be...*
*While the content generated may be factually incorrect, it can spur trains of thought.*
*Ride those rails as far as you can.*
*Another insight: ChatGPT is useful for generating concrete examples of abstractions.*

*This document will evolve, and I will check against non-AI souces as time allows.*

## A
**Alignment** - Ensuring a model’s behavior aligns with human intentions and ethical guidelines, critical in safe AI deployment. Here are three examples of alignment in generative AI:
1.	Content Moderation for Safe Responses: Alignment techniques are applied to ensure that a generative AI model avoids harmful or offensive content. For example, if asked a provocative question, an aligned AI model will respond in a neutral or informative way, steering clear of inappropriate or inflammatory responses.
2.	Values-Based Responses in Education Tools: An aligned AI used for educational purposes might be trained to encourage curiosity, empathy, and inclusivity in its responses. For example, if a student asks a sensitive question, the AI could respond in a supportive, respectful manner that aligns with educational values and promotes a positive learning environment.
3.	User-Specific Personalization and Assistance: When aligned with user goals, a generative AI assistant adapts its responses to the user’s specific needs. For instance, if a user seeks health advice, the model provides balanced, evidence-based information aligned with reputable health guidelines, ensuring advice is safe, relevant, and responsible.

**Artificial Intelligence (AI)** - The simulation of human intelligence in machines that can perform tasks typically requiring human intelligence, such as visual perception, speech recognition, decision-making, and language translation.

**Attention Mechanism** - A technique used in neural networks to focus on certain parts of an input sequence, especially useful in sequence-to-sequence tasks. (See Transformer.)

## B
**Bias** - Systematic errors or unfair advantages/disadvantages learned by a model, often due to unbalanced or incomplete training data. Here are three examples of bias in generative AI:
1.	Cultural and Gender Stereotypes: Generative language models trained on internet data can produce stereotypical descriptions or responses. For instance, when prompted with certain professions, the model might assume a specific gender, reinforcing cultural stereotypes (e.g., associating “nurse” with women or “engineer” with men).
2.	Underrepresentation of Minority Groups: Models trained on imbalanced datasets may produce content that reflects a narrow demographic focus, often favoring majority groups. For example, an image generation model trained mostly on Western faces might struggle to generate accurate images of people from diverse ethnic backgrounds, leading to underrepresentation or misrepresentation of minority groups.
3.	Political or Ideological Bias: Language models can inherit biases from training data that include dominant political or ideological perspectives. As a result, they may generate content that subtly or overtly favors certain viewpoints over others, potentially skewing information and creating echo chambers when used in news or opinion-based applications.

**Bias Mitigation** - Techniques used to reduce or eliminate biases within a model, often crucial for generative AI. Here are three examples of bias mitigation in generative AI:
1.	Balanced Training Data: To reduce biases, AI developers can create or curate training datasets that include diverse and representative examples from various demographics, cultures, and viewpoints. This approach helps ensure the model learns a more balanced perspective and minimizes the risk of favoring any single group or viewpoint.
2.	Adversarial Debiasing: This technique uses an additional model (or adversarial network) to detect and penalize biased patterns during training. By iteratively training the generative model to produce outputs that are evaluated for fairness by the adversarial model, the AI learns to reduce biased responses.
3.	Post-Processing Bias Filters: After generating content, the AI’s output can be analyzed using bias detection tools or filters that flag potentially biased or harmful responses. These filters can either modify or reject biased outputs, ensuring that the final result aligns with fairness and inclusivity standards.

## C

**Catastrophic forgetting** - also known as catastrophic interference, refers to a phenomenon in machine learning, particularly in neural networks, where a model that is trained sequentially on multiple tasks tends to forget the information from the earlier tasks when it learns new tasks. This issue is especially relevant in the context of generative AI, where a model needs to maintain the ability to generate data or perform tasks across multiple domains or categories without losing its previously acquired skills.

In generative AI, catastrophic forgetting can significantly hinder the model's performance because the generative model must retain complex distributions over data from various tasks. When the model is updated with new data or tasks, it may inadvertently overwrite or diminish the learned knowledge of prior tasks, resulting in a reduction in its overall capability and versatility.

To address catastrophic forgetting in generative AI, researchers have explored several approaches, such as:

1. **Regularization Techniques**: Methods like Elastic Weight Consolidation (EWC) introduce regularization terms to penalize changes in network parameters that are important for previous tasks.

2. **Replay Strategies**: These involve storing examples or prototypical examples of past tasks and periodically retraining the model on these examples to refresh its memory.

3. **Dynamic Architectures**: Using models that adapt their architectures dynamically by adding new nodes or modules to handle new tasks without affecting the performance on previous tasks.

4. **Meta-Learning & Transfer Learning**: Leveraging meta-learning frameworks or transferring learning frameworks to enable models to learn how to retain past knowledge effectively.

Addressing catastrophic forgetting is crucial for developing robust and capable generative AI systems that can handle lifelong learning across diverse tasks without continued retraining from scratch.

**Chain-of-Thought** (CoT) reasoning capabilities, allowing the AI to present its thought process in real time

**checkpoint**  a "checkpoint" generally refers to a saved state of a model during or after the training process. Here are a few aspects to consider:

1. **Model State**: Checkpoints save the model's weights and biases at a specific point in time, allowing you to resume training from that point without starting over.

2. **Version Control**: They can also act as a version control mechanism, where you can roll back to a previous state if subsequent training leads to undesirable outcomes, such as overfitting.

3. **Experimentation**: Checkpoints allow researchers and developers to experiment with different configurations, learning rates, and other hyperparameters by providing restore points.

4. **Deployment**: For deployment purposes, a checkpoint represents the final state of a model that's ready to be converted into a deployable format.

5. **Fault Tolerance**: In distributed training settings, checkpoints provide a way to recover from interruptions (like system failures) without significant data or compute loss.

Overall, checkpoints are critical for managing the training lifecycle of a model efficiently and effectively.

**Classifier-free guidance (CFG)** is a technique used in diffusion models to enhance the quality of generated images. This approach is particularly useful in generative models, such as those used for image synthesis. Here's a basic explanation of how it works:

1. **Diffusion Models**: These models generate images by iteratively refining random noise into a structured image. The process is guided by a learned model that predicts what the image should look like at each step.

2. **Guidance**: In the context of diffusion models, guidance helps in steering the generative process towards more desirable outputs. Classifier-free guidance specifically refers to a method where the guidance does not rely on an explicit classifier but uses intrinsic properties of the model.

3. **Latent Conditioning**: With classifier-free guidance, the model is conditioned not only on a noise vector but optionally on a conditioning input (such as class labels or other relevant data). This helps guide the diffusion process to generate images that adhere more closely to the desired outcome.

4. **Trade-off Parameter**: The guidance process often involves a trade-off parameter that balances the influence of the conditioning data against the inherent randomness of the noise process. Adjusting this parameter allows for manipulation of how close the output is to the conditioning input.

5. **Outcome**: By using classifier-free guidance, diffusion models can produce higher-quality images that are more consistent with the intended characteristics (like specific styles or objects) without needing a separate classifier model to steer the generation.

Classifier-free guidance is valued for its simplicity and effectiveness in generating detailed and coherent images using diffusion models, making it a powerful tool in generative artificial intelligence tasks.

What “CFG” (classifier‑free guidance) does, in plain language**

Imagine you have two artists working together:

1. **The “un‑guided” artist** – follows the model’s own guess about what the picture should look like, based purely on the random noise it sees.
2. **The “guided” artist** – also looks at the text prompt (or whatever condition you gave) and tries hard to make the picture match that prompt.

The CFG value tells the system **how much weight to give to the guided artist versus the un‑guided one**.

| CFG value | What the model does | Visible effect on the picture |
|-----------|---------------------|-------------------------------|
| **≈ 1** (or a little above) | Mostly listens to the un‑guided guess; the prompt only nudges the result a little. | The image may be softer, more “generic,” and sometimes drifts away from the exact prompt details. |
| **2 – 5** (common range) | Blends the two artists, giving a noticeable boost to the prompt‑following side. | The picture follows the prompt much more closely—objects, colors, and composition line up with what you asked for, while still keeping natural‑looking detail. |
| **> 5** (very strong guidance) | The guided artist dominates; the model aggressively forces the image to match the prompt. | The result can look “over‑styled” or “over‑sharpened.” You get the right objects, but textures may look artificial, edges may be too crisp, and subtle realism can disappear. |
| **< 1** (rare) | The model actually *subtracts* guidance, making the output deliberately less like the prompt. | The image becomes more random, dream‑like, or abstract. |

**Bottom line:**
- **Higher CFG → stronger adherence to the prompt, but risk of losing natural detail.**
- **Lower CFG → more creative freedom and realism, but the prompt may be ignored.**

Choosing a CFG in the 2‑5 range usually gives a good balance between “looks good” and “matches what you asked for.”

**CLIP**, which stands for Contrastive Language–Image Pretraining, is a model developed by OpenAI that plays a significant role in the context of generative AI. It is designed to understand and associate images and text in a way that allows for more advanced image and language processing tasks. CLIP is capable of performing zero-shot learning, meaning it can make predictions on tasks without direct prior examples, simply by understanding the context and relationships from the vast amount of data it was trained on.

In terms of functionality, CLIP works by learning visual concepts from natural language supervision. During training, it aligns images with corresponding textual descriptions, effectively creating a shared understanding or embedding space where both images and text can coexist. This capability makes CLIP particularly useful in tasks that require an understanding of visual and linguistic information simultaneously, such as image captioning, visual search, and integrated text-to-image generation.

In generative AI, CLIP is often paired with other models, such as DALL-E or other image generation models, to enhance the ability to generate or modify content based on textual input. By understanding the relationship between visuals and text, CLIP can guide these models to generate visual content that corresponds more accurately to specific textual prompts, improving the coherence and relevance of generated content.

**CLIP Vision** is the image‑encoding backbone of OpenAI’s Contrastive Language‑Image Pre‑training (CLIP) model. It takes an input image, splits it into patches (or processes it through a convolutional stem), and feeds the resulting token sequence into a transformer encoder. The encoder produces a high‑dimensional embedding that captures visual semantics—objects, scenes, textures, and relationships—in a way that aligns with the textual embeddings produced by the companion CLIP text encoder. By training on billions of (image, caption) pairs with a contrastive loss, CLIP Vision learns to place matching images and texts close together in a shared latent space, enabling zero‑shot image classification, retrieval, and cross‑modal tasks without any task‑specific fine‑tuning.

In a diffusion model the **“shift” value** is the deterministic component of the reverse‑diffusion step that moves a noisy sample \(x_t\) toward the estimated clean image.
When the model predicts either the added noise \(\epsilon_\theta(x_t,t)\) or the denoised latent \(\hat{x}_0\), the reverse transition is written as a Gaussian

\[
p_\theta(x_{t-1}\!\mid\!x_t) = \mathcal{N}\!\bigl(x_{t-1};\;\underbrace{\text{shift}}_{\mu_\theta(x_t,t)},\;\sigma_t^2 I\bigr),
\]

where the **shift** (the mean \(\mu_\theta\)) is a weighted combination of the current state \(x_t\) and the model’s prediction, typically

\[
\text{shift}= \frac{1}{\sqrt{\alpha_t}}\Bigl(x_t - \frac{\beta_t}{\sqrt{1-\bar\alpha_t}}\;\epsilon_\theta(x_t,t)\Bigr)
\quad\text{or}\quad
\text{shift}= \sqrt{\bar\alpha_{t-1}}\;\hat{x}_0 + \sqrt{1-\bar\alpha_{t-1}}\;\epsilon,
\]

with \(\alpha_t,\beta_t\) coming from the noise schedule.
Thus the shift tells the sampler **how far to “push” the sample toward the data manifold** at each timestep, while the remaining variance term adds the stochastic noise required for the diffusion process.

Think of a diffusion model as a painter who first splatters a canvas with random brush‑strokes (noise) and then, step by step, wipes away some of that mess to reveal the picture underneath.
The *shift* value is the amount of “wipe” the painter applies at each step. It tells the model how far to move the current noisy picture toward the clean image it thinks should be there.

**What you actually see**

- **Small shift (tiny wipe)** – The model only makes a slight adjustment each step, so the final picture looks fuzzy, blurry, or still contains remnants of the original noise. The image may be less sharp and may retain odd speckles or ghosted textures.
- **Large shift (big wipe)** – The model pulls the noisy canvas strongly toward the predicted clean image. The result is a clearer, more defined picture with crisp edges and less leftover grain. However, if the shift is too aggressive the painter can over‑correct, causing the image to lose subtle details or to look “over‑processed” (flat colors, loss of fine texture).

In short, the shift controls how aggressively the model cleans up the noise. Adjusting it changes the balance between a dreamy, noisy look and a sharp, clean output.

**Context Window** - The context window in generative AI refers to the maximum amount of text or data that a model can process at once when generating or analyzing content. Essentially, it defines how much information the model can “see” in a single pass, which affects its ability to understand and produce coherent responses based on previous parts of the text. For instance, if a model has a context window of 4,000 tokens, it can only consider the last 4,000 tokens as context for any response. A smaller context window limits the model’s ability to maintain consistency over long pieces of text, while a larger context window allows it to handle complex, multi-turn dialogues or lengthy documents without losing track of relevant details. Increasing the context window enhances the model’s performance on tasks requiring extensive contextual understanding, such as summarizing long articles or engaging in extended conversations.

The context window size and the prompt size are related but not the same. The context window size is the maximum amount of tokens (words, subwords, or characters) that the model can consider at once, including both the prompt (input from the user) and any generated output within a single interaction.

The prompt size, on the other hand, refers specifically to the portion of the context window taken up by the input text or instructions provided by the user. For example, if a model has a context window of 8,192 tokens and a prompt uses 3,000 tokens, then the model has 5,192 tokens left for generating responses. When the prompt and responses exceed the context window limit, the oldest parts of the conversation are typically removed or “forgotten” to make room for new content.

*Note: I asked ChatGPT 4o what its prompt size is... it could not disclose!*

## D
**Data Augmentation** - Techniques for increasing the diversity of a training dataset by modifying data in various ways, such as flipping images or altering text.

**Decoder** - Part of a neural network, typically in sequence-to-sequence models, that generates output from encoded data. See Transformer.

**Deep Learning** - A subset of ML using neural networks with many layers (deep neural networks) to model complex patterns in large datasets.

**Diffusion Model** - A type of generative model that learns to reverse a noise process to generate new data, often used for image synthesis.

Diffusion models are a class of generative machine‑learning models that create data (e.g., images, audio, or text) by gradually transforming simple random noise into a structured sample. They operate in two complementary phases:

1. **Forward (diffusion) process** – Starting from a real data point, the model repeatedly adds small amounts of Gaussian noise over many timesteps until the data is indistinguishable from pure noise. This process is analytically tractable and defines a known probability distribution at each step.

2. **Reverse (denoising) process** – A neural network is trained to predict and remove the added noise at each timestep, effectively learning the conditional distribution \(p(x_{t-1}\,|\,x_t)\). During generation, the model starts from pure noise and iteratively applies the learned denoising steps, reconstructing a realistic sample.

Key characteristics:

- **Probabilistic formulation** – The model learns a tractable variational lower bound on the data likelihood, allowing principled training.
- **High-quality generation** – Recent diffusion models (e.g., DALL‑E 2, Imagen, Stable Diffusion) produce images that rival or surpass GANs in fidelity and diversity.
- **Flexibility** – By conditioning the denoising network on text, class labels, or other modalities, diffusion models can perform guided synthesis, in‑painting, super‑resolution, and more.
- **Iterative refinement** – Generation requires many steps (often 50–1000), but recent research (e.g., DDIM, accelerated samplers) reduces this to a few dozen steps with little loss in quality.

In summary, diffusion models frame generation as a learned reverse‑diffusion of noise, combining strong theoretical foundations with state‑of‑the‑art results across multiple media types.

## E
**Embedding** - The representation of words, sentences, or other data types as vectors, enabling a model to process text in numerical form.

**Encoder** - An encoder in neural networks is a component that processes input data and transforms it into a compact, encoded representation. By extracting essential features and reducing dimensionality, the encoder makes it easier for the model to understand and process complex information, especially for tasks like classification, translation, and data compression.

**Ethics in AI** - Considerations around the societal, moral, and legal implications of deploying AI, especially generative models. ere are three examples of ethical issues in generative AI:
1.	Misinformation and Deepfakes: Generative AI can create hyper-realistic fake images, videos, or audio, often indistinguishable from real content. This can lead to misinformation or harm individuals’ reputations, especially when used maliciously to spread false information or fabricate events.
2.	Bias and Fairness: Generative AI models can learn biases present in their training data, leading to unfair or harmful outputs. For instance, a model trained on biased data might reinforce stereotypes or make discriminatory decisions in applications like hiring, law enforcement, or media representation.
3.	Intellectual Property and Copyright: Generative AI can create new content that resembles copyrighted material, raising questions about originality and ownership. This issue is significant for artists and content creators whose work may be replicated or imitated without credit or compensation.

## F
**Few-Shot Learning** - Training a model on a task with only a few examples, often seen in generative models that perform well with limited data.

**Fine-tuning** - The process of adjusting a pre-trained model on a smaller, task-specific dataset to improve its performance on that task.

**Foundation Model** - a "foundation model" refers to a large-scale, pre-trained machine learning model that serves as a general-purpose platform for a wide array of downstream tasks. These models are typically developed using vast datasets and extensive computational resources, allowing them to capture a broad understanding of language, vision, or other data types.

Foundation models are characterized by their versatility and ability to be fine-tuned or adapted to specific applications with relatively small amounts of additional data and computation. Examples include models like GPT-3 for natural language processing and other large-scale models for tasks such as image recognition or speech synthesis.

The key aspects of foundation models include:

1. **Scale**: They are usually very large, involving billions of parameters, enabling them to capture complex patterns and relationships in data.

2. **Pre-training**: They are trained on diverse and extensive datasets, which allows them to learn general features and patterns.

3. **Transferability**: Because of their broad training, they can be fine-tuned for specific tasks with less data and effort compared to training a new model from scratch.

Foundation models have become a cornerstone of AI development because they reduce the barrier to entry for AI deployment across different domains, making sophisticated AI capabilities more accessible.

**Frontier Model** - typically refers to the most advanced and capable AI models that push the boundaries of what is currently achievable in AI research and applications. These models are often characterized by their large scale, sophisticated architectures, and advanced training techniques, which enable them to perform complex tasks with high levels of performance.

Frontier models in generative AI are usually at the cutting edge of innovation, setting new benchmarks in the field. They are capable of generating increasingly coherent, contextually relevant, and high-quality content across various domains, such as natural language processing, computer vision, and audio synthesis.

Key features of frontier models often include:

1. **Scale**: They involve a large number of parameters, requiring substantial computational resources for training and fine-tuning.

2. **Architecture**: They utilize state-of-the-art architectures that build upon recent advancements in AI, such as transformer networks or diffusion models.

3. **Training Data**: They are trained on large and diverse datasets to generalize well across different scenarios and tasks.

4. **Multitask Capabilities**: They are designed to handle multiple tasks or modalities, such as generating text, images, or music.

5. **Performance**: They achieve superior performance metrics on benchmark tests compared to previous models, often setting new records in terms of accuracy, creativity, or realism.

6. **Generalization**: They exhibit strong generalization capabilities, allowing them to perform well on tasks for which they were not explicitly trained.

Frontier models are often developed by leading AI research labs and organizations, and they require collaboration across various disciplines, including computer science, linguistics, psychology, and ethics, to address challenges related to bias, fairness, and societal impact. These models play a significant role in advancing AI research and expanding the potential applications of AI technologies.

## G
**Generative Adversarial Network (GAN)** - A generative model framework with two neural networks (generator and discriminator) competing in a zero-sum game to improve data generation.

**Generative Model** - A type of model that learns to generate new data that resembles a training dataset, commonly used in creating images, text, audio, or video.

**Gradient Descent** - An optimization algorithm used to minimize a model’s error by iteratively adjusting weights in the neural network.

**GPT (Generative Pre-trained Transformer)** - A type of LLM developed by OpenAI that generates text by predicting subsequent words based on context.

## H
**Hallucination** - When a generative model produces false or misleading information confidently, despite lacking factual basis.

**Hyperparameters** - Configurable settings that control the model training process, such as learning rate, batch size, and number of layers.

## I
**Inpainting** - A technique where a model fills in missing parts of an image or text based on surrounding content. (see Outpainting)

Inpainting remains coherent with the rest of the image by using contextual information from surrounding pixels to generate new content that blends seamlessly with existing elements. During inpainting, the model analyzes the textures, colors, shapes, and edges in the areas adjacent to the missing or masked region. By learning these patterns, the model can predict and generate details that fit naturally into the scene, maintaining visual continuity. Advanced inpainting models, like those based on diffusion or GANs, are specifically trained to understand context and recognize common patterns within an image, enabling them to produce realistic and coherent extensions that align with the original style and content of the image.

## J
**Jailbreak** - refers to the process of manipulating or exploiting a language model or AI system to bypass its built-in safety measures, content filters, or ethical guidelines. This is typically done to generate content that the AI would normally restrict or refrain from producing, such as inappropriate, toxic, or biased outputs. Jailbreaking can pose ethical and security challenges, as it may lead to the spread of harmful information or misuse of AI technologies.

Jailbreaking strategies for generative AI aim to bypass content moderation and safety protocols. Here are some common types with examples:

1. **Prompt Engineering:**
   - *Strategy:* Crafting specific input prompts designed to trigger restricted outputs.
   - *Example:* If an AI is designed to avoid generating violent content, a user might structure a prompt around a historical reenactment or fictional scenario to coax the AI into generating similar content indirectly.

2. **Obfuscated Language or Code:**
   - *Strategy:* Using indirect language, symbols, or coding to elicit restricted responses.
   - *Example:* To bypass restrictions on certain topics, users might use analogies, metaphors, or code words that aren't explicitly recognized by the AI's filters.

3. **Context Shifting:**
   - *Strategy:* Providing a broad and seemingly benign context that gradually shifts towards restricted content.
   - *Example:* Starting a conversation about a fantasy scenario and slowly guiding the discussion toward restricted themes by building on each of the AI’s preceding responses.

4. **Role-Playing:**
   - *Strategy:* Engaging the AI in a role-playing scenario where it adopts a character, potentially bypassing its restrictions.
   - *Example:* Asking the AI to act like a character from history who lived in a time of war, thereby encouraging it to discuss conflicts and strategies outside its usual content limits.

5. **Chaining or Iterative Prompting:**
   - *Strategy:* Using a series of related prompts to build up to a forbidden topic gradually.
   - *Example:* Expressing curiosity about related non-restricted topics first, and then asking open-ended questions that guide the conversation toward the desired output.

6. **Manipulative Framing:**
   - *Strategy:* Presenting prompts in a manner that suggests an academic or critical analysis need.
   - *Example:* Requesting information on controversial topics under the guise of needing it for a research paper or debate preparation, potentially leading the AI to relax its filters under the assumption of a legitimate request.

## L
**Large Language Model (LLM)** - A model trained on vast amounts of text data, enabling it to generate human-like language and understand context. Examples include GPT, BERT, and T5.

"Broadly speaking, LLMs utilize a transformer architecture to (1) add semantic meaning to various types of data, capturing context and relationships within sequences—whether they are words in text, patches in images, frames in videos or samples in audio—and (2) are built on neural networks to model these relationships for generating outputs appropriate to the medium, such as text, images, video or songs."

**Latent Space** - A representation space where a model encodes input data, allowing relationships and patterns to be mapped in lower dimensions.

## M
**Machine Learning (ML)** - A subset of AI focused on using algorithms and statistical models to enable computers to perform specific tasks without explicit programming by learning from data.

**Multimodal Model** - A model that can process and generate multiple types of data, such as text and images, often simultaneously.

## N
**Natural Language Processing (NLP)** - A field of AI focused on the interaction between computers and human language, enabling tasks like language translation, text generation, and sentiment analysis.

**Neural Network** - A model inspired by the human brain’s network of neurons, consisting of layers of nodes (neurons) that can process and recognize patterns in data.

**Node** - In neural networks, nodes (or neurons) correspond to units in each layer that process input data, while parameters include the weights and biases that control each node’s behavior. Each connection between nodes has an associated weight parameter, which determines the strength and direction of the connection’s influence on the next layer. Additionally, each node often has a bias parameter that shifts its output, providing flexibility in how each node processes information.

Together, weights and biases are the adjustable parameters learned during training. They define how data flows through the network, and their values are iteratively optimized to minimize error, allowing the network to recognize patterns and make accurate predictions.

A single node (or neuron) in a neural network typically has one bias parameter and multiple weights—one weight for each connection to nodes in the previous layer. So, if a node receives inputs from  n  nodes in the previous layer, it will have:
	•	 n  weight parameters (one for each input connection)
	•	1 bias parameter

In total, a node will have  n + 1  parameters. For example, if there are 5 connections from the previous layer to this node, it will have 5 weights and 1 bias, totaling 6 parameters.

**Noise** - In the context of diffusion models, noise refers to random disturbances added to an image or data sample in a step-by-step process, effectively degrading it until it becomes nearly unrecognizable. During training, the model learns to reverse this “diffusion” of noise, predicting and reconstructing each previous step to gradually generate coherent data from pure noise. By learning this process, the model becomes capable of generating new, high-quality images or data by reversing the noise addition process from a random starting point.

## 0

**Outpainting** - A technique in generative AI that extends an image beyond its original borders by generating new, visually coherent content that blends seamlessly with the existing image. This process uses context from the original image to imagine and fill in surrounding areas, allowing for creative expansion and completion of visual scenes. (see Inpainting)

**Overfitting** - Overfitting occurs when a model learns the training data too well, capturing not only the underlying patterns but also the noise and outliers specific to that dataset. While this might make the model perform exceptionally well on training data, it often leads to poor performance on new, unseen data because the model struggles to generalize beyond the exact examples it has learned. In essence, an overfitted model becomes too specialized, lacking the flexibility needed to adapt to variations in real-world data. To mitigate overfitting, techniques such as regularization, dropout, early stopping, and using larger, more varied datasets are employed, helping the model to focus on general patterns rather than memorizing specifics.

## P
**Parameter** - (see Node.)

**Personalization** - Adjusting a model’s outputs to suit individual users’ preferences or characteristics, enhancing user experience.

**Positional Encoding** - (see Transformer.)

**Pre-training** - The process of training a model on a large dataset in an unsupervised manner before fine-tuning on a specific task.

**Prompt** - An initial input or question used to instruct or guide a generative AI model to produce specific outputs.

**Prompt injection** - Prompt injection refers to a type of security vulnerability specific to natural language processing systems and AI models that use prompts to guide their behavior or generate responses. In a prompt injection attack, an adversary crafts a particular input to manipulate the model's output or behavior in unintended ways.

This can be done by injecting unexpected instructions or misleading information into a prompt or context that the model uses to generate its response. For example, if an AI is instructed to follow commands or answer questions based on a given prompt, an attacker might prepend or append additional instructions or questions intended to deceive or alter the model's intended response behavior.

The risk of prompt injection arises because the AI models often do not have the capability to distinguish between legitimate user instructions and malicious content within their training or operational methodologies. Consequently, ensuring robust input validation and implementing safeguards to detect and prevent such manipulations are essential to mitigating the risks associated with prompt injections.

## R
**Retrieval-Augmented Generation (RAG)** - This approach combines retrieval-based techniques with generative models to improve the quality and accuracy of generated content. The main idea is to enhance the generative model by incorporating relevant information retrieved from an external knowledge base or data source. Here’s a breakdown of the process:

1. **Retrieval**: A retrieval model is used to search and extract the most relevant pieces of information from a large set of data (such as a database, documents, or internet resources) based on a given input or query.

2. **Augmentation**: The retrieved information is then used to enrich or augment the input given to the generative model. This step ensures that the generative model has access to accurate and contemporaneous data that may not be part of its original training.

3. **Generation**: Finally, a generative model, such as a transformer-based language model, uses the augmented input to produce a response or generate content. The incorporation of retrieved data helps the model generate content that is more informed and contextually relevant.

RAG is particularly useful for tasks that require up-to-date knowledge or specific factual information, enhancing the ability of generative models to provide accurate and context-aware results.

**Reinforcement Learning from Human Feedback (RLHF)** - A training method that incorporates human preferences to improve model responses. It involves training machine learning models by incorporating evaluations and preferences from humans to guide and improve the learning process. This approach is particularly valuable for tasks where specifying a clear reward function is challenging, but human evaluators can easily distinguish between more and less desirable outcomes.

In generative AI, RLHF is used to align the outputs of models with human expectations and preferences. Here's how it typically works:

1. **Model Generation**: A generative model produces outputs based on initial training from a dataset. These outputs might not always align well with human expectations or desires.

2. **Human Feedback**: Human evaluators review these outputs and provide feedback. This feedback can be in the form of rankings, ratings, or qualitative comments indicating the quality or preferences concerning the outputs generated by the model.

3. **Reinforcement Learning**: Using this feedback, a reinforcement learning algorithm adjusts the model's parameters. The algorithm is designed to optimize future outputs to be more aligned with human feedback, essentially learning what types of outputs are preferred by humans.

4. **Iteration**: The process is iterative, and through successive rounds of feedback and adjustment, the model improves over time, producing outputs that better satisfy human evaluators.

RLHF is especially useful in scenarios where traditional supervised learning might struggle to capture complex, subjective qualities like aesthetic appeal, humor, or ethical considerations. It helps in ensuring that generative models not only produce technically correct outputs but also ones that are contextually appropriate and aligned with human values.

**Residual vector quantization (RVQ)** - a technique used in the context of signal processing and data compression, which can also find applications in generative AI, particularly in the compression and efficient representation of high-dimensional data. It is an extension of basic Vector Quantization (VQ), aiming to improve the quality of quantization by reducing quantization errors.

In a generative AI context, RVQ can be used to efficiently encode and decode data, such as images, audio, or other types of high-dimensional data points generated by models. Here's how it works:

1. **Initial Quantization**: The data vector is first approximated using a coarse quantization. This initial quantization captures the broad structure of the data but may leave significant quantization errors or residuals.

2. **Residual Calculation**: The difference between the original data vector and the approximated version from the initial quantization is computed. This difference is called a residual vector.

3. **Sequential Quantization of Residuals**: These residual vectors are then further quantized using additional VQ stages. Each stage attempts to encode the residual error from the previous stage. By quantizing the residuals sequentially, the approximation becomes increasingly refined.

4. **Iterative Process**: This process of quantizing residuals can be repeated multiple times. Each subsequent stage operates on the residuals of the previous stage, continually reducing the error until the desired level of fidelity is reached or until computational constraints limit further processing.

In generative AI, RVQ can be advantageous for:

- **Data Compression**: Efficiently compressing and decompressing data by breaking it down into components, which helps in reducing storage and bandwidth requirements.

- **Improving Model Efficiency**: Helping generative models like GANs (Generative Adversarial Networks) or VAEs (Variational Autoencoders) to better represent and generate high-dimensional data with lower error rates.

- **Progressive Refinement**: RVQ allows for progressive refinement of generated data, which can be useful in applications where it is beneficial to quickly generate a rough approximation and iteratively refine it.

Thus, RVQ provides a method to achieve more efficient and precise data representation in generative AI tasks, facilitating better output quality and resource management.


## S
**Sampling** - The process of generating new data points by selecting from a probability distribution produced by a generative model.

**Seed** - In generative AI, a seed is an initial input value used to set or control the randomness of the model’s output. Seeds ensure reproducibility, meaning that if the same model, prompt, and seed are used, the model will produce identical results each time. This is particularly useful in creative tasks, such as image generation or story generation, where consistency might be desired across multiple runs. By changing the seed, users can influence the model to generate different variations, allowing exploration of diverse outputs from the same input prompt. In essence, the seed acts as a starting point for random sampling, enabling both controlled experimentation and creative variability in AI-generated content.

*Using the same seed, but altering other parameters by increments, is a useful method for uderstanding the impact of changes in other parameters.*

**Self-Attention** - A mechanism where each part of an input sequence interacts with all other parts to understand dependencies, used widely in transformers. (see Transformer.)

**SFT, or Supervised Fine-Tuning,** - is a process in the development of machine learning models, particularly natural language processing models like those in the GPT series. After an initial pre-training phase, where a model learns patterns in data through unsupervised learning, it often undergoes further fine-tuning. This involves supervised fine-tuning, where the model is trained with labeled datasets for specific tasks.

### Key Aspects of SFT:

1. **Purpose**: The goal of supervised fine-tuning is to refine and enhance the model's performance on specific tasks by providing it with more targeted examples. This helps align the model closer to desired outputs.

2. **Data**: During SFT, the model is exposed to datasets where each input is paired with a labeled response or output. These datasets help the model understand how to map inputs to correct outputs more accurately.

3. **Applications**: It's often used to adapt a general-purpose language model to perform specific tasks, such as sentiment analysis, translation, or other specific language understanding tasks.

4. **Benefits**: Fine-tuning helps improve the model's accuracy, reliability, and performance on specialized tasks by reducing overfitting on general data patterns not relevant to the task at hand.

Supervised fine-tuning is essential in making a broadly trained AI model more adept at handling specific, practical language tasks, enhancing its utility in real-world applications.

## T
**Temperature** - In the context of generative AI, temperature is a parameter that controls the randomness and creativity of the model’s responses. Lower temperature values (close to 0) make the model more deterministic and focused, often leading to precise and conservative outputs as it favors the most probable or predictable choices. Higher temperature values (approaching 1 or above) introduce more randomness, enabling the model to explore a wider range of possible words or phrases, which can lead to more creative, varied, or unexpected responses. For example, a higher temperature might be used for tasks requiring originality, like storytelling, while a lower temperature might be preferred for factual tasks, like answering technical questions. Adjusting temperature helps tailor the model’s behavior to suit different use cases, balancing between coherence and creativity.

**Token** - A token is a unit of data, such as a word, subword, or character, that a model processes individually. In natural language processing, tokens are created by breaking down text into smaller, manageable parts, enabling the model to analyze and generate language effectively. Tokenization converts text into a sequence of tokens, each with a unique identifier, allowing the model to understand and work with language systematically.

**Tokenization** - The process of splitting text into smaller units, or tokens, for a model to process.

Audio data is tokenized by breaking down the audio waveform into small, manageable segments or features that can be processed by a model. This often involves:

1.	Segmenting the Audio: The continuous audio signal is split into short, fixed-length time frames (typically a few milliseconds each), capturing local patterns and fluctuations in sound.

2.	Extracting Features: For each frame, features like Mel-frequency cepstral coefficients (MFCCs), spectrogram values, or log-Mel spectrograms are extracted. These features represent the audio’s frequency and intensity over time, creating a structured, numerical representation that retains meaningful information.

3.	Encoding as Tokens: Each frame or feature vector becomes a “token” that the model processes sequentially, similar to how text is tokenized into words or subwords.

This approach allows models to understand patterns in audio, such as phonemes in speech, which can then be used for tasks like speech recognition, synthesis, or classification.

Images are tokenized by dividing them into smaller, structured units that a model can process in sequence. Common methods include:

1.	Dividing into Patches: The image is split into small, fixed-size patches (e.g., 16x16 pixels), treating each patch as a single token. This approach is used in models like Vision Transformers (ViTs), where each patch represents a distinct part of the image and is embedded as a vector.

2.	Flattening Pixels or Using Grids: In some cases, each pixel or a grid of pixels is treated as an individual token, especially for detailed image analysis. Each pixel or grid region’s RGB or grayscale values form a feature vector, creating a structured representation.

3.	Applying Feature Extraction: Convolutional neural networks (CNNs) or other feature extractors can convert image regions into high-level feature vectors, which can be tokenized as representations of shapes, textures, or edges.

Tokenizing images in this way allows models to process visual information systematically, enabling tasks like image recognition, segmentation, and generation by understanding patterns within and between image regions.

**Training** (in Generative AI) is the process of teaching a model to generate new data by learning patterns and features from a given dataset. During training, the model processes input data, adjusts its internal parameters, and minimizes error using optimization techniques (like gradient descent) to improve its ability to recreate or extend patterns in the data. This involves feeding the model large datasets and iterating over them multiple times so it can learn underlying structures and relationships. The result is a model that can generate new, similar data, whether it’s images, text, or other forms of content, based on the learned patterns.

Here are some best practices for gathering high-quality training data for machine learning models:

1.	Ensure Diversity and Representativeness: Include a wide range of examples that represent all relevant demographics, use cases, and scenarios to prevent biases and enhance model generalization. For example, if training a language model, gather data from various sources, dialects, and languages to capture diverse linguistic patterns.

2.	Data Quality and Accuracy: Use reliable sources and verify the accuracy of your data to ensure the model learns correct information. Avoid using noisy or poorly labeled data, as this can degrade model performance. Cleaning and preprocessing data to remove inconsistencies, duplicates, and errors are essential steps.

3.	Ethical and Privacy Compliance: Adhere to ethical guidelines and legal standards (e.g., GDPR) when collecting data, especially if it includes personal or sensitive information. Anonymize and secure data wherever possible to protect individual privacy.

4.	Balance Across Categories: Strive for balanced data across relevant categories to prevent the model from being skewed toward overrepresented groups. For example, in a sentiment analysis model, ensure equal representation of positive, neutral, and negative sentiments to avoid a bias in predictions.

5.	Labeling Consistency: Use consistent and accurate labeling practices, ideally with multiple labelers for quality assurance. Implement a standardized guideline for labelers to minimize subjectivity and discrepancies.

6.	Use Real-World Data: When possible, gather data that closely resembles the scenarios in which the model will be used. This will help the model learn relevant patterns and perform effectively in practical applications.

7.	Maintain Transparency and Documentation: Document the data collection process, including the sources, any preprocessing steps, and the rationale behind the selection criteria. This transparency aids in understanding any limitations or biases in the data and allows for informed adjustments later.

8.	Data Augmentation: To increase dataset diversity and quantity, consider data augmentation techniques, such as generating slight variations in the data (e.g., flipping images, paraphrasing sentences) to improve model robustness without collecting entirely new data.

**Transfer learning** - the technique where a model developed for a particular task is reused as the starting point for a model on a second task. This is particularly useful in generative AI, where creating complex models from scratch requires significant computational resources and large amounts of data.

In generative AI, transfer learning typically involves taking a pre-trained model that has been developed on a large dataset and adapting it for a different, but related task. For example, a generative model trained on the ImageNet dataset for image generation could be fine-tuned on a smaller dataset of medical images. The model leverages the knowledge it has gained from the first task (like recognizing complex patterns and features in images) to improve the performance on the second task without needing to start training from scratch.

Transfer learning is beneficial because:

1. **Efficiency**: It reduces the time and computational resources needed since the base layers of the network, which capture general patterns, do not need retraining.

2. **Performance**: It often leads to better performance, especially when data for the target task is limited, as the model can leverage knowledge from the source domain.

3. **Robustness**: The model starts with more generalized features, which can make it more robust and adaptable to new tasks.

Overall, transfer learning helps in building efficient, effective generative AI models by reusing and adapting existing models to new and related challenges.

**Transformer** - Transformers are a type of neural network architecture that excels at processing sequential data, like text, by focusing on relationships within the data rather than processing it one step at a time, as recurrent neural networks (RNNs) do. Developed in 2017 by researchers at Google Brain, the transformer architecture introduced a breakthrough in handling long-range dependencies and understanding context, making it foundational for many modern language models, including BERT, GPT, and T5.

Here’s a breakdown of the key concepts and processes that make transformers work:

Self-Attention Mechanism - Central to the transformer is the self-attention mechanism, which allows each word (or token) in a sequence to “pay attention” to every other word, weighing their importance relative to the current word. This is crucial because it helps the model understand context more accurately. For example, in the sentence “The cat sat on the mat, and it was soft,” the self-attention mechanism helps the model understand that “it” refers to “the mat,” not “the cat.” Self-attention calculates three vectors for each word: Query, Key, and Value. By calculating the similarity between the Query of one word and the Key of another, the model determines how much attention to give to other words and produces a weighted sum of their Values. This weighted sum helps the model grasp relationships and dependencies in the data.

Positional Encoding - Transformers don’t process tokens sequentially. To help the model understand the order of words (important for natural language understanding), transformers use positional encoding. Positional encodings are added to each token’s embedding, so the model knows whether a word appears first, last, or somewhere in between in a sequence. Positional encodings allow transformers to make sense of word order even without traditional sequence-based processing, making them more effective at capturing global context.

Multi-Head Attention - Self-attention in transformers is divided into multiple attention heads (multi-head attention). Each head performs its own self-attention calculation on the input sequence, allowing the model to focus on different aspects of the sequence in parallel. For instance, one head might focus on relationships related to subject-verb agreement, while another might focus on object relationships. By combining the outputs from each head, the transformer gains a richer understanding of context and relationships across the sequence.

Encoder-Decoder Structure - The transformer model is structured in layers, with each layer containing both self-attention and feed-forward neural networks. It is typically divided into encoder and decoder components: The encoder layers process the input data, learning internal representations that capture the input sequence’s contextual meanings. The decoder layers generate output sequences (e.g., a translation or response) based on the encoder’s output. Each decoder layer also includes attention mechanisms that allow it to focus on relevant parts of the encoded input sequence. For tasks like translation or summarization, the encoder processes the input sentence while the decoder generates the output sentence, one token at a time.

Feed-Forward Networks and Layer Normalization - Each transformer layer also contains a feed-forward neural network applied to each token individually and identically. This network further processes the token representations learned by self-attention. To help the network train efficiently, transformers use layer normalization after each attention and feed-forward sub-layer, normalizing outputs to make learning more stable and prevent issues from exploding or vanishing gradients.

Parallelization and Scalability - Unlike RNNs that process sequences step-by-step, transformers process the entire sequence simultaneously, making them highly parallelizable and more computationally efficient. This enables them to scale well on powerful hardware like GPUs and TPUs. This parallelism allows transformers to handle much longer input sequences than previous models, significantly boosting performance on long-text understanding, generation, and summarization.

Training and Transfer Learning - Transformers are typically trained on massive datasets in an unsupervised manner, using techniques like masked language modeling (e.g., BERT) or autoregressive training (e.g., GPT). After this pre-training, transformers can be fine-tuned on smaller, task-specific datasets for applications like question answering, translation, or chatbots. This transfer learning capability makes transformers highly versatile.

Transformers have become foundational in natural language processing (NLP), powering applications like:
* Language Translation (e.g., Google Translate uses transformer models for accurate translations),
* Text Summarization (transformers can condense information from long documents),
* Question Answering (by understanding context to answer questions accurately),
* Text Generation (generating coherent, contextually relevant responses in chatbots and virtual assistants).

Transformers revolutionized how we handle sequential data, especially text, by focusing on attention mechanisms over traditional sequence-based processing. This design has led to significant advancements in generative AI, enabling models that produce increasingly realistic and contextually relevant outputs.

## V
**Variational Autoencoder (VAE)** - A generative model that encodes data to a lower-dimensional space and decodes it, allowing the generation of new data instances that resemble the training data.

What a Variational Auto‑Encoder (VAE) does when it receives a prompt

A VAE is fundamentally an **encoder ↔ decoder** pair trained with a probabilistic loss (the ELBO).
When you give it a *prompt*—for example, a piece of text, a sketch, a partially‑filled image, or any other conditioning signal—the VAE processes it in roughly the following steps:

| Step | What Happens | Why It Matters |
|------|--------------|----------------|
| **1. Conditioning (optional)** | The prompt is first transformed (e.g., embedded with a language model, passed through a small CNN, or concatenated with a learned embedding) so that the encoder and decoder can “see” it. | This tells the VAE *what* kind of data you are interested in generating or reconstructing. |
| **2. Encoding** | The encoder network consumes the prompt (or the prompt‑plus‑input data) and produces two vectors: a mean **μ** and a log‑variance **log σ²** for each latent dimension. | These parameters define a Gaussian posterior **q(z | x, prompt)** that captures the uncertainty about the hidden representation. |
| **3. Re‑parameterization** | A random sample **ε ∼ 𝒩(0, I)** is drawn and combined with the encoder outputs: <br> **z = μ + σ · ε** (where σ = exp(0.5 · log σ²)). | This trick makes the sampling step differentiable, allowing the whole model to be trained end‑to‑end. |
| **4. Decoding (generation)** | The decoder receives the latent sample **z** (often concatenated with the prompt’s conditioning vector) and produces an output distribution **p(x̂ | z, prompt)**. For images this might be a pixel‑wise Gaussian; for text a categorical distribution over tokens. | The decoder translates the compact latent code back into the original data space, guided by the prompt. |
| **5. Sampling / Reconstruction** | - **Reconstruction mode**: If you supplied a full input (e.g., an image) together with a prompt, the decoder’s most likely output **x̂** is taken as the reconstruction. <br> - **Generation mode**: If you only gave a prompt, the encoder may be bypassed; you simply sample **z ∼ 𝒩(0, I)** (or from a learned prior) and let the decoder generate a new sample conditioned on the prompt. | In reconstruction you get a denoised version of the original; in generation you obtain a novel piece of data that respects the prompt’s constraints. |
| **6. Loss (training only)** | The model is trained to minimise the ELBO: <br> **L = reconstruction loss + KL( q(z|x,prompt) ‖ p(z) )**. | The reconstruction term forces fidelity to the input, while the KL term regularises the latent space, keeping it close to the prior and enabling smooth sampling. |

#### In a nutshell
- **Prompt → Conditioning vector** (optional)
- **Encoder → μ, σ** (posterior over latents)
- **Re‑parameterization → z** (sampled latent)
- **Decoder + Prompt → output** (reconstruction or new generation)

Because the latent space is learned to be continuous and roughly Gaussian, you can also *interpolate* between prompts, *edit* a prompt and see gradual changes in the output, or *sample* many diverse results from the same prompt by drawing different **z** values. This makes VAEs especially useful for tasks like **guided image synthesis, text‑to‑image generation, style transfer, and semi‑supervised learning**.

## Z
**Zero-Shot Learning** - When a model performs a task without being explicitly trained on it by leveraging general understanding from diverse training data.
